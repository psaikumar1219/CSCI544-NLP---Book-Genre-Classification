{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6835935c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba0ae7f",
   "metadata": {},
   "source": [
    "# 1. DATASET GENERATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab496a2",
   "metadata": {},
   "source": [
    "## Reading data into a dataframe. We use the pandas read_table method and give the dataset name. We also use the property \"on_bad_lines = skip\" which does not take the rows which probably might not follow the same format as others and not be favorable for the further steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "53f0f0c3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_table('data.tsv', on_bad_lines='skip')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0b3afd",
   "metadata": {},
   "source": [
    "## Taking only two columns. We only take the columns \"star_rating\" and \"review_body\" which we use for the further processing. We simply do not consider other columns and hence do not include in this dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a18e60bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df[[\"star_rating\",\"review_body\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1d3d533",
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = df2[df2['star_rating'] == '1']\n",
    "c2 = df2[df2['star_rating'] == '2']\n",
    "frames = [c1,c2]\n",
    "class1 = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5ae81f",
   "metadata": {},
   "source": [
    "## We only take the random 20000 rows from the data having ratings 1 and 2. We use sample() to choose the random 20000 rows from the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23946228",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls1 = class1.sample(20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5720110",
   "metadata": {},
   "source": [
    "## We only take the random 20000 rows from the data having rating 3. We use sample() to choose the random 20000 rows from the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44a1d5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "c3 = df2[df2['star_rating'] == '3']\n",
    "cls2 = c3.sample(20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a9ab42",
   "metadata": {},
   "source": [
    "## We only take the random 20000 rows from the data having ratings 4 and 5. We use sample() to choose the random 20000 rows from the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d648e097",
   "metadata": {},
   "outputs": [],
   "source": [
    "c4 = df2[df2['star_rating'] == '4']\n",
    "c5 = df2[df2['star_rating'] == '5']\n",
    "frames1 = [c4,c5]\n",
    "class3 = pd.concat(frames1)\n",
    "cls3 = class3.sample(20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4c06853",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames2 = [cls1,cls2,cls3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c46d67",
   "metadata": {},
   "source": [
    "## We take all the dataframes with 20000 rows having each class into a new dataframe with 60000 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3fbf2747",
   "metadata": {},
   "outputs": [],
   "source": [
    "samp = pd.concat(frames2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5596bf65",
   "metadata": {},
   "source": [
    "## Creating a new column \"classification\". We populate the classification column with class labels which the functions returns for a rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15784ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_category(row):\n",
    "    if row['star_rating'] == '1' or row['star_rating'] == '2':\n",
    "        val = 1\n",
    "    elif row['star_rating'] == '3':\n",
    "        val = 2\n",
    "    else:\n",
    "        val = 3\n",
    "    return val\n",
    "\n",
    "samp['classification'] = samp.apply(class_category, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c29e0d",
   "metadata": {},
   "source": [
    "## We clean the data to a certain extent similar to HW1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bbcc46c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "samp['review_body'] = samp['review_body'].str.replace('http[s]?://(?:[a-z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', ' ')\n",
    "samp['review_body'] = samp['review_body'].str.replace(r'<[^<>]*>', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc3f9537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple/\r\n",
      "Requirement already satisfied: contractions in ./opt/miniconda3/lib/python3.9/site-packages (0.1.73)\r\n",
      "Requirement already satisfied: textsearch>=0.0.21 in ./opt/miniconda3/lib/python3.9/site-packages (from contractions) (0.0.24)\r\n",
      "Requirement already satisfied: pyahocorasick in ./opt/miniconda3/lib/python3.9/site-packages (from textsearch>=0.0.21->contractions) (2.0.0)\r\n",
      "Requirement already satisfied: anyascii in ./opt/miniconda3/lib/python3.9/site-packages (from textsearch>=0.0.21->contractions) (0.3.1)\r\n"
     ]
    }
   ],
   "source": [
    "import sys  \n",
    "!{sys.executable} -m pip install contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77d776b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import contractions\n",
    "def cont_to_exp(x):\n",
    "    if type(x) is str:\n",
    "        x = x.replace(x, contractions.fix(x))\n",
    "        return x\n",
    "    else:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f65800f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "samp['review_body'] = samp['review_body'].apply(lambda x : cont_to_exp(x))\n",
    "samp = samp.apply(lambda x: x.astype(str).str.lower())\n",
    "samp['review_body'] = samp['review_body'].str.replace('[^a-z]', ' ')\n",
    "samp['review_body'] = samp['review_body'].str.replace('  ', ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f255b59f",
   "metadata": {},
   "source": [
    "# 2. WORD EMBEDDING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45a6d2c",
   "metadata": {},
   "source": [
    "#### References\n",
    "https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e22c07e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple/\n",
      "Requirement already satisfied: gensim in ./opt/miniconda3/lib/python3.9/site-packages (4.3.0)\n",
      "Requirement already satisfied: numpy>=1.18.5 in ./opt/miniconda3/lib/python3.9/site-packages (from gensim) (1.24.1)\n",
      "Requirement already satisfied: scipy>=1.7.0 in ./opt/miniconda3/lib/python3.9/site-packages (from gensim) (1.10.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in ./opt/miniconda3/lib/python3.9/site-packages (from gensim) (6.3.0)\n",
      "Requirement already satisfied: FuzzyTM>=0.4.0 in ./opt/miniconda3/lib/python3.9/site-packages (from gensim) (2.0.5)\n",
      "Requirement already satisfied: pyfume in ./opt/miniconda3/lib/python3.9/site-packages (from FuzzyTM>=0.4.0->gensim) (0.2.25)\n",
      "Requirement already satisfied: pandas in ./opt/miniconda3/lib/python3.9/site-packages (from FuzzyTM>=0.4.0->gensim) (1.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in ./opt/miniconda3/lib/python3.9/site-packages (from pandas->FuzzyTM>=0.4.0->gensim) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./opt/miniconda3/lib/python3.9/site-packages (from pandas->FuzzyTM>=0.4.0->gensim) (2022.7)\n",
      "Requirement already satisfied: simpful in ./opt/miniconda3/lib/python3.9/site-packages (from pyfume->FuzzyTM>=0.4.0->gensim) (2.10.0)\n",
      "Requirement already satisfied: fst-pso in ./opt/miniconda3/lib/python3.9/site-packages (from pyfume->FuzzyTM>=0.4.0->gensim) (1.8.1)\n",
      "Requirement already satisfied: six>=1.5 in ./opt/miniconda3/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas->FuzzyTM>=0.4.0->gensim) (1.16.0)\n",
      "Requirement already satisfied: miniful in ./opt/miniconda3/lib/python3.9/site-packages (from fst-pso->pyfume->FuzzyTM>=0.4.0->gensim) (0.0.6)\n",
      "Requirement already satisfied: requests in ./opt/miniconda3/lib/python3.9/site-packages (from simpful->pyfume->FuzzyTM>=0.4.0->gensim) (2.27.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./opt/miniconda3/lib/python3.9/site-packages (from requests->simpful->pyfume->FuzzyTM>=0.4.0->gensim) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in ./opt/miniconda3/lib/python3.9/site-packages (from requests->simpful->pyfume->FuzzyTM>=0.4.0->gensim) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./opt/miniconda3/lib/python3.9/site-packages (from requests->simpful->pyfume->FuzzyTM>=0.4.0->gensim) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./opt/miniconda3/lib/python3.9/site-packages (from requests->simpful->pyfume->FuzzyTM>=0.4.0->gensim) (1.26.8)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61dca7f1",
   "metadata": {},
   "source": [
    "## We load the pre-trained Google News word embedding model using the gensim library. The model is trained on a large corpus of news articles and contains 300-dimensional word embeddings for over 3 million words and phrases. Once loaded, the model can be used to find the most similar words to a given word or set of words, among other things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d087c498",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5b9bd9",
   "metadata": {},
   "source": [
    "## We create a list of word embeddings for each review. We loop over the 'review_body' column in 'samp' (dataframe) and for each review it iterates over the words in the review. If the word is present in the vocabulary of the pre-trained Word2Vec model it retrieves the corresponding word embedding and appends it to a list called 'sample'. Finally, the list of word embeddings for each review is appended to a list called 'embeddings'. Therefore, at the end of the loop, embeddings is a list of lists, where each inner list contains the word embeddings for each word in the corresponding review that is present in the Word2Vec model vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6b8db518",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = []\n",
    "for review in samp['review_body']:\n",
    "    sample = []\n",
    "    for word in review:\n",
    "        if word in wv.key_to_index:\n",
    "            word_embed = wv[word]\n",
    "            sample.append(word_embed)\n",
    "    embeddings.append(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f226979a",
   "metadata": {},
   "source": [
    "## (2a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b67538",
   "metadata": {},
   "source": [
    "### Checking the semantic similarities between the words using the similarity method of the Word2Vec. The similarity method takes two word strings as input and returns a floating-point number representing the cosine similarity between the two word vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4013daeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "excellent - outstanding:  0.55674857\n",
      "woman - girl:  0.7494641\n",
      "summer - winter:  0.7155519\n"
     ]
    }
   ],
   "source": [
    "print(\"excellent - outstanding: \", wv.similarity('excellent','outstanding'))\n",
    "print(\"woman - girl: \", wv.similarity('woman','girl'))\n",
    "print(\"summer - winter: \", wv.similarity('summer','winter'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1c58f865",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "great - cheap + quality:  [('tremendous', 0.5572595000267029), ('terrific', 0.5461479425430298), ('fantastic', 0.5364927053451538), ('wonderful', 0.525030255317688), ('excellent', 0.4941006004810333), ('exceptional', 0.4823820888996124), ('marvelous', 0.4571710228919983), ('excellence', 0.4563022553920746), ('phenomenal', 0.4542866349220276), ('incredible', 0.4519862234592438)]\n",
      "comfortable - uncomfortable + fit:  [('fits', 0.6162262558937073), ('complement', 0.478362113237381), ('suited', 0.46157076954841614), ('fits_perfectly', 0.4559015929698944), ('shape', 0.45541536808013916), ('flawlessly_Hyun', 0.4369184076786041), ('fitted', 0.4197128117084503), ('fits_nicely', 0.4167429506778717), ('tailored', 0.4000203013420105), ('comfortably', 0.3961005210876465)]\n",
      "birthday - birth + wedding:  [('birthday_bash', 0.6283815503120422), ('##st_birthday', 0.593088686466217), ('wedding_anniversary', 0.5861030220985413), ('bridal_shower', 0.5464682579040527), ('wed_ding', 0.5427249073982239), ('bachelorette_party', 0.5324557423591614), ('bday', 0.5217258334159851), ('housewarming_party', 0.5133200287818909), ('birthday_celebrations', 0.5122348070144653), ('Valentine_Day', 0.5107334852218628)]\n"
     ]
    }
   ],
   "source": [
    "print(\"great - cheap + quality: \", wv.most_similar(positive=['great','quality'],negative=['cheap']))\n",
    "print(\"comfortable - uncomfortable + fit: \", wv.most_similar(positive=['comfortable','fit'],negative=['uncomfortable']))\n",
    "print(\"birthday - birth + wedding: \", wv.most_similar(positive=['birthday','wedding'],negative=['birth']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28274622",
   "metadata": {},
   "source": [
    "## (2b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540f647c",
   "metadata": {},
   "source": [
    "### To train a Word2Vec model on our dataset, I first tokenized the sentences in each review using the 'split()' method, and appended each of the words to a list (r_list)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8bff652c",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_list = []\n",
    "for r in samp['review_body']:\n",
    "    r_list.append(r.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87d822a",
   "metadata": {},
   "source": [
    "### The Word2Vec constructor is used to create a model with the given arguments: word embedding size 300, window size 13, and minimum word count 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "752bef3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "wvmodel = Word2Vec(sentences=r_list,vector_size=300,window=13,min_count=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2077a350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "excellent - outstanding:  0.72037846\n",
      "woman - girl:  0.6619442\n",
      "summer - winter:  0.6729769\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print(\"excellent - outstanding: \", wvmodel.wv.similarity('excellent','outstanding'))\n",
    "except:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    print(\"woman - girl: \", wvmodel.wv.similarity('woman','girl'))\n",
    "except:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    print(\"summer - winter: \", wvmodel.wv.similarity('summer','winter'))\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5ff89191",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "great - cheap + quality:  [('excellent', 0.5716205835342407), ('fantastic', 0.48812922835350037), ('wonderful', 0.4656205475330353), ('taming', 0.44979894161224365), ('priced', 0.4329557716846466), ('amazingly', 0.4292263388633728), ('luxurious', 0.4230978786945343), ('outstanding', 0.42294615507125854), ('terrific', 0.4081841707229614), ('awesome', 0.40585049986839294)]\n",
      "comfortable - uncomfortable + fit:  [('case', 0.7120247483253479), ('design', 0.7044004797935486), ('fits', 0.6879077553749084), ('travel', 0.6656880378723145), ('holder', 0.65728759765625), ('compact', 0.6516857743263245), ('headband', 0.6506212949752808), ('adjustable', 0.650370180606842), ('combs', 0.6488838195800781), ('shape', 0.6487677693367004)]\n",
      "birthday - birth + wedding:  [('christmas', 0.7250697016716003), ('granddaughter', 0.6815987825393677), ('sister', 0.6681199669837952), ('sisters', 0.6397242546081543), ('niece', 0.6295629739761353), ('party', 0.6288538575172424), ('gift', 0.6172934174537659), ('mother', 0.614536464214325), ('mom', 0.6031947731971741), ('themed', 0.5973345041275024)]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print(\"great - cheap + quality: \", wvmodel.wv.most_similar(positive=['great','quality'],negative=['cheap']))\n",
    "except:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    print(\"comfortable - uncomfortable + fit: \", wvmodel.wv.most_similar(positive=['comfortable','fit'],negative=['uncomfortable']))\n",
    "except:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    print(\"birthday - birth + wedding: \", wvmodel.wv.most_similar(positive=['birthday','wedding'],negative=['birth']))\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65656fca",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "### After comparing the vectors generated by the pre-trained Word2Vec model and the model trained on the dataset, it is evident that the pre-trained model encodes the similarities between words more accurately. This is expected since the pre-trained model is trained on massive datasets compared to the meagre dataset of 60,000 entries used for training the custom model. However, it is worth noting that in certain contexts, such as reviews, words like \"excellent\" and \"outstanding\" may have greater significance, and the custom model may produce better encodings. In conclusion, while models trained on large datasets generally generate better encodings, there may be cases where the context of the data impacts the encoding accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd465a3",
   "metadata": {},
   "source": [
    "# 3. SIMPLE MODELS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39869066",
   "metadata": {},
   "source": [
    "## We find the average vectors for each review. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f5e2e9",
   "metadata": {},
   "source": [
    "### We iterate through the dataframe 'samp' and split each review into words. We check if the word is present in the 'wv'. If yes, we append the vector value of that particular word into the 'vec' list. We find the sum of the vectors in the vec list and divide it by the length of the the vec list to get the average and append it to list x, and, subsequently, we empty the vec list for the next review. For every review, we also take the corresponding class label into another list y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ea5f8618",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=[]\n",
    "y=[]\n",
    "\n",
    "for i, row in samp.iterrows():\n",
    "    vec = []\n",
    "    for word in row['review_body'].split():\n",
    "        if word in wv.key_to_index:\n",
    "            vec.append(wv[word])\n",
    "    if vec:\n",
    "        vec = sum(vec) / len(vec)\n",
    "        x.append(vec)\n",
    "        y.append(row['classification'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b5d62b",
   "metadata": {},
   "source": [
    "### printMetrics() function prints the precision, recall, f1-score, and their respective averages for each class label. It takes the testing sample of the class labels and output of the predict() method in each model as parameters, and prints the precision, recall, f1-score separated by comma. It gives the average of the above metrics in the last line. First we get a classification report in the dictionary form and we store its transpose in a dataframe. We iterate through the dictionary and take only the metrics we need.¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "32ef8273",
   "metadata": {},
   "outputs": [],
   "source": [
    "def printMetrics(y_test, label):\n",
    "    cr = classification_report(y_test, label, output_dict=True)\n",
    "    report = pd.DataFrame(cr).transpose()\n",
    "    for i in range(4):\n",
    "        if i==3:\n",
    "            print(f'Averages: {report.iloc[i+1][\"precision\"]}, {report.iloc[i+1][\"recall\"]}, {report.iloc[i+1][\"f1-score\"]}\\n')\n",
    "        else:\n",
    "            print(f'Class {i+1}: {report.iloc[i][\"precision\"]}, {report.iloc[i][\"recall\"]}, {report.iloc[i][\"f1-score\"]}\\n')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e81025",
   "metadata": {},
   "source": [
    "### Splitting the dataset into test and train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3271c221",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.2,stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4d08af",
   "metadata": {},
   "source": [
    "## Simple models using Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa423e65",
   "metadata": {},
   "source": [
    "### Implementing Perceptron using Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "718c9c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 1: 0.4622128920721166, 0.9371557336004006, 0.6190869996692028\n",
      "\n",
      "Class 2: 0.7092198581560284, 0.100150225338007, 0.1755155770074594\n",
      "\n",
      "Class 3: 0.7768045907580792, 0.6454203262233376, 0.7050438596491228\n",
      "\n",
      "Averages: 0.649412446995408, 0.5609087617205818, 0.4998821454419284\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "model = Perceptron()\n",
    "model.fit(x_train, y_train)\n",
    "labelPredict = model.predict(x_test)\n",
    "accuracy_score(y_test, labelPredict)\n",
    "\n",
    "printMetrics(y_test, labelPredict) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc0ba4d",
   "metadata": {},
   "source": [
    "### Accuracy for the Perceptron model using Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a0fe895a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Perceptron - Word2Vec:  0.5608452351123361\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy for Perceptron - Word2Vec: \", accuracy_score(y_test, labelPredict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4f4744",
   "metadata": {},
   "source": [
    "### Implementing SVM using Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1167c993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 1: 0.6597569692637598, 0.6932899349023536, 0.6761079233304846\n",
      "\n",
      "Class 2: 0.5859375, 0.5633450175262894, 0.5744191983660966\n",
      "\n",
      "Class 3: 0.7492378048780488, 0.7400250941028859, 0.7446029541724529\n",
      "\n",
      "Averages: 0.6649774247139362, 0.665553348843843, 0.6650433586230114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "SVMmodel = LinearSVC()\n",
    "SVMmodel.fit(x_train, y_train)\n",
    "SVMLabelPredict = SVMmodel.predict(x_test)\n",
    "accuracy_score(y_test, SVMLabelPredict)\n",
    "\n",
    "printMetrics(y_test, SVMLabelPredict) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca148c01",
   "metadata": {},
   "source": [
    "### Accuracy for the SVM model using Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bfdb607b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for SVM - Wrod2Vec:  0.665497369080431\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy for SVM - Wrod2Vec: \", accuracy_score(y_test, SVMLabelPredict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1274948f",
   "metadata": {},
   "source": [
    "## Simple models using TF-IDF vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae759d0",
   "metadata": {},
   "source": [
    "### Here we import the tfidfVectorizer to convert the reviews into a matrix of TFIDF features. The ngram_range would give us the range of n_grams to be included in the Bag of Words. (1,3) would give us n_grams from one to three words. X is the matrix of TFIDF features from the reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b3bb0904",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,3))\n",
    "\n",
    "X = vectorizer.fit_transform(samp['review_body'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba41fce8",
   "metadata": {},
   "source": [
    "### Splitting the dataset into test and train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f6c44b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "xt_train, xt_test, yt_train, yt_test = train_test_split(X,samp['classification'],test_size = 0.2,stratify=samp['classification'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd7ccd8",
   "metadata": {},
   "source": [
    "### Implementing Perceptron using TF-IDF vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "acdfdb5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 1: 0.7307186101605686, 0.694, 0.7118861392486217\n",
      "\n",
      "Class 2: 0.631497683993824, 0.6135, 0.6223687547552625\n",
      "\n",
      "Class 3: 0.7756662804171495, 0.83675, 0.8050511124473841\n",
      "\n",
      "Averages: 0.7126275248571806, 0.71475, 0.7131020021504227\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "tfidfmodel = Perceptron()\n",
    "tfidfmodel.fit(xt_train, yt_train)\n",
    "tfidflabelPredict = tfidfmodel.predict(xt_test)\n",
    "accuracy_score(yt_test, tfidflabelPredict)\n",
    "\n",
    "printMetrics(yt_test, tfidflabelPredict) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef7b570",
   "metadata": {},
   "source": [
    "### Accuracy for the Perceptron model using TF-IDF Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a48571f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Perceptron - TF-IDF:  0.71475\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy for Perceptron - TF-IDF: \", accuracy_score(yt_test, tfidflabelPredict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf59f0b",
   "metadata": {},
   "source": [
    "### Implementing SVM using TF-IDF Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "324f2e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 1: 0.7384729796727814, 0.74475, 0.7415982076176251\n",
      "\n",
      "Class 2: 0.6608276212236608, 0.65075, 0.6557500944703364\n",
      "\n",
      "Class 3: 0.8281599205363794, 0.83375, 0.830945558739255\n",
      "\n",
      "Averages: 0.7424868404776072, 0.7430833333333334, 0.7427646202757389\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "tfidfSVMmodel = LinearSVC()\n",
    "tfidfSVMmodel.fit(xt_train, yt_train)\n",
    "tfidfSVMLabelPredict = tfidfSVMmodel.predict(xt_test)\n",
    "accuracy_score(yt_test, tfidfSVMLabelPredict)\n",
    "\n",
    "printMetrics(yt_test, tfidfSVMLabelPredict) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c02861",
   "metadata": {},
   "source": [
    "### Accuracy for the SVM model using TF-IDF Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f91ebab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for SVM - TF-IDF:  0.7430833333333333\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy for SVM - TF-IDF: \", accuracy_score(yt_test, tfidfSVMLabelPredict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68cd178e",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "### The results clearly demonstrate that TF-IDF features outperform Word2Vec features by a significant margin. The accuracies achieved by Perceptron and SVM models with TF-IDF features are 0.71475 and 0.7430833333333333 respectively, while the accuracies achieved by Word2Vec features are 0.5608452351123361 and 0.665497369080431 respectively. This suggests that TF-IDF features are better suited for sentiment analysis as they take into account the frequency of occurrence of each word, whereas Word2Vec features average the vectors and may cause important words to lose their prominence, resulting in lower accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c363356",
   "metadata": {},
   "source": [
    "# 4. FEEDFORWARD NEURAL NETWORKS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c7fede",
   "metadata": {},
   "source": [
    "#### References\n",
    "https://www.kaggle.com/mishra1993/pytorch-multi-layer-perceptron-mnist <br>\n",
    "https://www.youtube.com/watch?v=oPhxf2fXHkQ&ab_channel=PatrickLoeber"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c0cea8",
   "metadata": {},
   "source": [
    "## Installing and importing the necessary libraries and packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ceba432b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple/\n",
      "Requirement already satisfied: torch in ./opt/miniconda3/lib/python3.9/site-packages (1.13.1)\n",
      "Requirement already satisfied: typing-extensions in ./opt/miniconda3/lib/python3.9/site-packages (from torch) (4.5.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0eb38062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple/\n",
      "Requirement already satisfied: torchvision in ./opt/miniconda3/lib/python3.9/site-packages (0.14.1)\n",
      "Requirement already satisfied: numpy in ./opt/miniconda3/lib/python3.9/site-packages (from torchvision) (1.24.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./opt/miniconda3/lib/python3.9/site-packages (from torchvision) (9.4.0)\n",
      "Requirement already satisfied: requests in ./opt/miniconda3/lib/python3.9/site-packages (from torchvision) (2.27.1)\n",
      "Requirement already satisfied: typing-extensions in ./opt/miniconda3/lib/python3.9/site-packages (from torchvision) (4.5.0)\n",
      "Requirement already satisfied: torch==1.13.1 in ./opt/miniconda3/lib/python3.9/site-packages (from torchvision) (1.13.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in ./opt/miniconda3/lib/python3.9/site-packages (from requests->torchvision) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./opt/miniconda3/lib/python3.9/site-packages (from requests->torchvision) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./opt/miniconda3/lib/python3.9/site-packages (from requests->torchvision) (1.26.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./opt/miniconda3/lib/python3.9/site-packages (from requests->torchvision) (3.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "021b9a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch.nn.utils.rnn as rnn_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010daa6c",
   "metadata": {},
   "source": [
    "## We take the x and y lists which contain the average Word2Vec vectors and corresponding class labels respectively and convert them to PyTorch 'tensor' objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d525c831",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x = torch.tensor(x)\n",
    "y = torch.tensor([int(label) for label in y])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b5038e",
   "metadata": {},
   "source": [
    "## (4a) Splitting the x & y data into train data and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "58165d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.2,stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0ddd19",
   "metadata": {},
   "source": [
    "### Since the labels in our dataframe are one-indexed, we make them zero-indexed and fit the standard format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "56a1195d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(y_train)):\n",
    "    y_train[i] = int(y_train[i]) - 1\n",
    "for i in range(len(y_test)):\n",
    "    y_test[i] = int(y_test[i]) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34906da1",
   "metadata": {},
   "source": [
    "## Fully connected neural network with two hidden layers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3f2e5b",
   "metadata": {},
   "source": [
    "### The '__init__()' initializes the feedforward neural network with the given input, hidden, and output sizes. It creates the necessary layers for the network, including an input layer, two hidden layers, and an output layer. The input size is the number of features in the input data, the hidden size is the number of neurons in the hidden layers, and the output size is the number of classes in the classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2e4bfd29",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neuralNet(\n",
      "  (l1): Linear(in_features=300, out_features=100, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (l2): Linear(in_features=100, out_features=10, bias=True)\n",
      "  (l3): Linear(in_features=10, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class neuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(neuralNet, self).__init__()\n",
    "        self.l1 = nn.Linear(300,100)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.l2 = nn.Linear(100,10)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.l3 = nn.Linear(10,3)\n",
    "    \n",
    "    def forward(self,t):\n",
    "        t = torch.tensor(t)\n",
    "        t = self.l1(t)\n",
    "        t = torch.relu(t)\n",
    "        t = self.l2(t)\n",
    "        t = torch.relu(t)\n",
    "        t = self.l3(t)\n",
    "        return t\n",
    "sam_model = neuralNet()\n",
    "\n",
    "print(sam_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aca34c9",
   "metadata": {},
   "source": [
    "### Here we use Cross-Entropy loss function, which is commonly used in multi-class classification problems. This loss function is particularly useful when the classes are not mutually exclusive, meaning that an instance can belong to more than one class. \n",
    "### We use Stochastic Gradient Descent (SGD) optimizer with a learning rate (lr) of 0.007. SGD is an optimization algorithm commonly used for training neural networks. It updates the model's parameters by computing the gradients of the loss function with respect to each parameter and adjusting the parameters in the opposite direction of the gradient. The learning rate determines the step size of these updates and can have a significant impact on the model's performance during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a2afbacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(sam_model.parameters(), lr=0.007)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6a03f5",
   "metadata": {},
   "source": [
    "### The model is trained for a fixed number of epochs (50) and for each epoch, the training data is divided into batches of a fixed size (32). For each batch, the model predicts the output based on the input and the current model parameters, and the cross entropy loss is calculated between the predicted output and the true output. The gradients of the loss with respect to the model parameters are then computed using backpropagation, and the optimizer updates the parameters based on the gradients. The code also prints the loss for every 10 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ac93311f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.1015831232070923\n",
      "Epoch 10, Loss: 0.9192432165145874\n",
      "Epoch 20, Loss: 0.8440624475479126\n",
      "Epoch 30, Loss: 0.7760268449783325\n",
      "Epoch 40, Loss: 0.7942439913749695\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 50\n",
    "bsize = 32\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for item in range(0,len(x_train),bsize):\n",
    "        output = sam_model(x_train[item:item+bsize])\n",
    "\n",
    "        loss = criterion(output, torch.tensor(y_train[item:item+bsize]))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    if (epoch%10 == 0):\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e6e250",
   "metadata": {},
   "source": [
    "### To Evaluate the performance of a neural network model on a test set by making predictions and comparing them with the true labels. It does this by first disabling gradient tracking using the torch.no_grad() context manager. Then, it passes the test inputs through the model and obtains the predicted class labels by taking the index of the maximum output value along the 1st dimension. These predicted labels are then compared with the true labels to calculate the classification accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0f06bdf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for FNN: 0.6806982377014951\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    output = sam_model(x_test)\n",
    "    _, predicted = torch.max(output.data,1)\n",
    "#     print(type(y_test))\n",
    "    \n",
    "    accuracy = (predicted == torch.tensor(y_test)).sum().item()/len(y_test)\n",
    "    print(f\"Accuracy for FNN: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3676ca84",
   "metadata": {},
   "source": [
    "## (4b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c72055",
   "metadata": {},
   "source": [
    "### We take the reviews and check if they are longer than 10 words. If yes, we slice it off to be only 10 words. If no, we add the empty strings to make it at least 10 words. Then, we check if the word is present in the 'wv'. If yes, we append the respective 300 vectors to the 'temp' list. If no, we add [0]\\*300 to make up for the missing words and to make sure that each entry is of size 3000. In each iteration, we convert the 'temp' list to a numpy array and concatenate the 'temp' to 'vectors' list. We also take the corresponding class labels into the 'labels' list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "209a2811",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = []\n",
    "labels = []\n",
    "for i,row in samp.iterrows():\n",
    "    words = row['review_body'].split()\n",
    "    if len(words) >= 10:\n",
    "        words = words[:10]\n",
    "    else:\n",
    "        words += [''] * (10 - len(words))\n",
    "\n",
    "    temp = []\n",
    "    for i, word in enumerate(words):\n",
    "        if word in wv.key_to_index:\n",
    "            temp.append(wv[word])\n",
    "        else:\n",
    "            temp.append([0]*300)\n",
    "\n",
    "    temp = np.array(temp)\n",
    "    vectors.append(temp.flatten())\n",
    "    temp = []\n",
    "    labels.append(row['classification'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fde7a1a",
   "metadata": {},
   "source": [
    "## We take the vectors and labels lists which contain the average Word2Vec vectors and corresponding class labels respectively and convert them to PyTorch 'tensor' objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6f5f5297",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = torch.tensor(vectors)\n",
    "labels = torch.tensor([int(label) for label in labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2528af",
   "metadata": {},
   "source": [
    "### Splitting the train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d8a07d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "p_train, p_test, q_train, q_test = train_test_split(vectors,labels,test_size = 0.2,stratify=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e8d8a3",
   "metadata": {},
   "source": [
    "### Since the labels in our dataframe are one-indexed, we make them zero-indexed and fit the standard format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ce715ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(q_train)):\n",
    "    q_train[i] = int(q_train[i]) - 1\n",
    "for i in range(len(q_test)):\n",
    "    q_test[i] = int(q_test[i]) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1092b6c2",
   "metadata": {},
   "source": [
    "### The '__init__()' initializes the feedforward neural network with the given input, hidden, and output sizes. It creates the necessary layers for the network, including an input layer, two hidden layers, and an output layer. The input size is the number of features in the input data, the hidden size is the number of neurons in the hidden layers, and the output size is the number of classes in the classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7eb031a2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neuralNet(\n",
      "  (l1): Linear(in_features=3000, out_features=100, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (l2): Linear(in_features=100, out_features=10, bias=True)\n",
      "  (l3): Linear(in_features=10, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class neuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(neuralNet, self).__init__()\n",
    "        self.l1 = nn.Linear(3000,100)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.l2 = nn.Linear(100,10)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.l3 = nn.Linear(10,3)\n",
    "    \n",
    "    def forward(self,t):         \n",
    "        t = self.l1(t)\n",
    "        t = torch.relu(t)\n",
    "        t = self.l2(t)\n",
    "        t = torch.relu(t)\n",
    "        t = self.l3(t)\n",
    "        return t\n",
    "    \n",
    "sam_ten_model = neuralNet()\n",
    "\n",
    "print(sam_ten_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d950b90e",
   "metadata": {},
   "source": [
    "### Here we use Cross-Entropy loss function, which is commonly used in multi-class classification problems. This loss function is particularly useful when the classes are not mutually exclusive, meaning that an instance can belong to more than one class. \n",
    "### We use Stochastic Gradient Descent (SGD) optimizer with a learning rate (lr) of 0.007. SGD is an optimization algorithm commonly used for training neural networks. It updates the model's parameters by computing the gradients of the loss function with respect to each parameter and adjusting the parameters in the opposite direction of the gradient. The learning rate determines the step size of these updates and can have a significant impact on the model's performance during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2d583384",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(sam_ten_model.parameters(), lr=0.007)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c78592",
   "metadata": {},
   "source": [
    "### The model is trained for a fixed number of epochs (50) and for each epoch, the training data is divided into batches of a fixed size (32). For each batch, the model predicts the output based on the input and the current model parameters, and the cross entropy loss is calculated between the predicted output and the true output. The gradients of the loss with respect to the model parameters are then computed using backpropagation, and the optimizer updates the parameters based on the gradients. The code also prints the loss for every 10 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "584521be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.087114930152893\n",
      "Epoch 10, Loss: 0.6032682061195374\n",
      "Epoch 20, Loss: 0.5234559774398804\n",
      "Epoch 30, Loss: 0.39046531915664673\n",
      "Epoch 40, Loss: 0.1485455483198166\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 50\n",
    "bsize = 32\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for item in range(0,len(p_train),bsize):        \n",
    "        output = sam_ten_model(p_train[item:item+bsize].to(torch.float32))\n",
    "        loss = criterion(output, torch.tensor(q_train[item:item+bsize]))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    if (epoch%10 == 0):\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1c7ae9",
   "metadata": {},
   "source": [
    "### To Evaluate the performance of a neural network model on a test set by making predictions and comparing them with the true labels. It does this by first disabling gradient tracking using the torch.no_grad() context manager. Then, it passes the test inputs through the model and obtains the predicted class labels by taking the index of the maximum output value along the 1st dimension. These predicted labels are then compared with the true labels to calculate the classification accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8314f8f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.5866666666666667\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    output = sam_ten_model(p_test.to(torch.float32))\n",
    "    _, predicted = torch.max(output.data,1)\n",
    "#     print(type(y_test))\n",
    "    \n",
    "    accuracy = (predicted == torch.tensor(q_test)).sum().item()/len(q_test)\n",
    "    print(f\"Test accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4a25b1",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "### In the context of sentiment analysis, it is clear that the MLP model utilizing Word2Vec features outperforms the simple models. While both models take average vectors of Word2Vec features, the MLP model achieves a higher accuracy of 0.6806982377014951 compared to any simple model. This indicates that multi-layered, fully connected neural networks have a better ability to predict output class labels than single-layer models like the Perceptron.\n",
    "### On the other hand, limiting the review length to only the first 10 words results in a low accuracy of 0.5866666666666667 when using the same MLP model. This is likely because important keywords such as \"good,\" \"bad,\" or \"excellent\" may be ignored, potentially altering the output class prediction and leading to a low accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a51ecee",
   "metadata": {},
   "source": [
    "# 5. RECURRENT NEURAL NETWORKS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e1e873",
   "metadata": {},
   "source": [
    "#### References\n",
    "https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html<br>\n",
    "https://www.youtube.com/watch?v=WEV61GmmPrk&ab_channel=PatrickLoeber<br>\n",
    "https://www.youtube.com/watch?v=0_PgWWmauHk&ab_channel=PatrickLoeber<br>\n",
    "https://www.simplilearn.com/tutorials/deep-learning-tutorial/rnn<br>\n",
    "https://www.analyticsvidhya.com/blog/2022/01/tutorial-on-rnn-lstm-gru-with-implementation/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89418544",
   "metadata": {},
   "source": [
    "### We take the reviews and check if they are longer than 10 words. If yes, we slice it off to be only 20 words. If no, we add the empty strings to make it at least 20 words. Then, we check if the word is present in the 'wv'. If yes, we append the respective 300 vectors to the 'temp_five' list. If no, we add [0]\\*300 to make up for the missing words and to make sure that each entry is of size 6000. In each iteration, we convert the 'temp_five' list to a numpy array and concatenate the 'temp_five' to 'vectors_five' list. We also take the corresponding class labels into the 'labels_five' list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6df9bd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors_five = []\n",
    "labels_five = []\n",
    "for i,row in samp.iterrows():\n",
    "    words = row['review_body'].split()\n",
    "    \n",
    "    if len(words) >= 20:\n",
    "        words = words[:20]\n",
    "    else:\n",
    "        words += [''] * (20 - len(words))\n",
    "\n",
    "    temp_five = []\n",
    "    for i, word in enumerate(words):\n",
    "        if word in wv.key_to_index:\n",
    "            temp_five.append(wv[word])\n",
    "        else:\n",
    "            temp_five.append([0]*300)\n",
    "\n",
    "    temp_five = np.array(temp_five)\n",
    "    vectors_five.append(temp_five.flatten())\n",
    "    temp_five = []\n",
    "    labels_five.append(row['classification'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b838e7e9",
   "metadata": {},
   "source": [
    "## We take the vectors_five and labels_five lists which contain the average Word2Vec vectors and corresponding class labels respectively and convert them to PyTorch 'tensor' objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fbe9bb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors_five = torch.tensor(vectors_five)\n",
    "labels_five = torch.tensor([int(label) for label in labels_five])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485ded53",
   "metadata": {},
   "source": [
    "### Splitting the data into train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0fac27a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "pt_train, pt_test, qt_train, qt_test = train_test_split(vectors_five,labels_five,test_size = 0.2,stratify=labels_five)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a013f1a",
   "metadata": {},
   "source": [
    "### Since the labels in our dataframe are one-indexed, we make them zero-indexed and fit the standard format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e214fac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(qt_train)):\n",
    "    qt_train[i] = int(qt_train[i]) - 1\n",
    "for i in range(len(qt_test)):\n",
    "    qt_test[i] = int(qt_test[i]) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be6ec8b",
   "metadata": {},
   "source": [
    "## (5a) Recurrent Neural Networks (RNN) is a type of neural network that can process sequential data by allowing information to persist from previous inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be794aa9",
   "metadata": {},
   "source": [
    "### Here we define the RNN using PyTorch. The RNN has one layer with a specified input size (=6000), hidden size (=20), and output size(=3). The input data is expected to have a batch size and is fed through the RNN using the forward method. The output from the RNN is then passed through a fully connected layer with the output size specified. The model uses the Cross Entropy Loss as its criterion and the Stochastic Gradient Descent optimizer to update the parameters during training. The model is trained for a specified number of epochs with a given batch size and learning rate. Finally, the model is moved to the available device (GPU or CPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6abf4305",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, batch_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size = batch_size\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(1, self.batch_size, self.hidden_size).to(device) \n",
    "        out, hn = self.rnn(x, h0) \n",
    "        out = self.fc(out[:, -1, :]) \n",
    "        return out\n",
    "\n",
    "    \n",
    "input_size = 6000 \n",
    "hidden_size = 20\n",
    "output_size = 3 \n",
    "lr = 0.007\n",
    "num_epochs = 100\n",
    "batch_size = 32\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "rnn = RNN(input_size, hidden_size, output_size, batch_size).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(rnn.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66464aa9",
   "metadata": {},
   "source": [
    "### The model is trained for a fixed number of epochs (100) and for each epoch, the training data is divided into batches of a fixed size (32). For each batch, the model predicts the output based on the input and the current model parameters, and the cross entropy loss is calculated between the predicted output and the true output. The gradients of the loss with respect to the model parameters are then computed using backpropagation, and the optimizer updates the parameters based on the gradients. The code also prints the loss for every 10 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "491fc786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 1.0943\n",
      "Epoch [11/100], Loss: 0.7971\n",
      "Epoch [21/100], Loss: 0.7307\n",
      "Epoch [31/100], Loss: 0.7010\n",
      "Epoch [41/100], Loss: 0.6843\n",
      "Epoch [51/100], Loss: 0.6712\n",
      "Epoch [61/100], Loss: 0.6590\n",
      "Epoch [71/100], Loss: 0.6461\n",
      "Epoch [81/100], Loss: 0.6324\n",
      "Epoch [91/100], Loss: 0.6188\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for i in range(0, len(pt_train), batch_size):\n",
    "        batch_data = pt_train[i:i+batch_size].to(torch.float32)\n",
    "        batch_labels = qt_train[i:i+batch_size].to(torch.long)\n",
    "#         batch_data = [data for data in batch_data]\n",
    "        inputs = nn.utils.rnn.pad_sequence(batch_data, batch_first=True, padding_value=0).to(device) # pad sequences to max length\n",
    "        lab = torch.tensor(batch_labels).to(device)\n",
    "#         inputs = torch.tensor(inputs)\n",
    "        inputs = inputs.reshape(batch_size, -1, input_size)\n",
    "        outputs = rnn(inputs)\n",
    "        loss = criterion(outputs, lab)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333b2248",
   "metadata": {},
   "source": [
    "### Here we perform validation on the test set using a trained RNN model. It loops through the test set in batches of size \"batch_size\", and for each batch, it converts the data and labels to the appropriate tensor format, pads the sequences to the max length, and feeds them into the RNN model. The outputs are then compared against the true labels, and the number of correct predictions is accumulated. Finally, the accuracy is calculated as a percentage of correctly classified samples over the total number of samples, and printed out. The \"with torch.no_grad()\" block ensures that no gradients are calculated during the validation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7834a617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on RNN: 0.6148\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i in range(0, len(pt_test), batch_size):\n",
    "        batch_data = pt_test[i:i+batch_size].to(torch.float32)\n",
    "        batch_labels = qt_test[i:i+batch_size].to(torch.long)\n",
    "        inputs = nn.utils.rnn.pad_sequence(batch_data, batch_first=True, padding_value=0).to(device) # pad sequences to max length\n",
    "        lab = torch.tensor(batch_labels).to(device)\n",
    "        inputs = inputs.reshape(batch_size, -1, input_size)\n",
    "        outputs = rnn(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += lab.size(0)\n",
    "        correct += (predicted == lab).sum().item()\n",
    "\n",
    "print(f'Accuracy on RNN: {correct / total:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27fe2757",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "### After experimenting with the simple RNN model, we observed that limiting the review length to 20 words resulted in an accuracy of 0.6148. However, similar to the previous case, this approach has the potential to miss out on important keywords that could affect the output class prediction. Although, we noticed an improvement in accuracy when compared to the case where we considered only the first 10 words, the accuracy is still lower than that of the MLP model considering the full review."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae3831d",
   "metadata": {},
   "source": [
    "## (5b) GRU (Gated Recurrent Unit) is a type of neural network architecture that is widely used in natural language processing tasks, specifically in tasks that require modeling sequential data. It is a variant of the recurrent neural network (RNN) that uses gating mechanisms to better capture long-term dependencies in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c01c19a",
   "metadata": {},
   "source": [
    "### Here we define the GRU using PyTorch. The GRU has one layer with a specified input size (=6000), hidden size (=20), and output size(=3). The input data is expected to have a batch size and is fed through the RNN using the forward method. The output from the GRU is then passed through a fully connected layer with the output size specified. The model uses the Cross Entropy Loss as its criterion and the Stochastic Gradient Descent optimizer to update the parameters during training. The model is trained for a specified number of epochs with a given batch size and learning rate. Finally, the model is moved to the available device (GPU or CPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "88da5731",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, batch_size):\n",
    "        super(GRU, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size = batch_size\n",
    "        self.gru = nn.GRU(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(1, self.batch_size, self.hidden_size).to(device) \n",
    "        out, hn = self.gru(x, h0) \n",
    "        out = self.fc(out[:, -1, :]) \n",
    "        return out\n",
    "\n",
    "input_size = 6000 \n",
    "hidden_size = 20\n",
    "output_size = 3 \n",
    "lr = 0.007\n",
    "num_epochs = 100\n",
    "batch_size = 32\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "gru = GRU(input_size, hidden_size, output_size, batch_size).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(gru.parameters(), lr=lr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38aad8c",
   "metadata": {},
   "source": [
    "### The model is trained for a fixed number of epochs (100) and for each epoch, the training data is divided into batches of a fixed size (32). For each batch, the model predicts the output based on the input and the current model parameters, and the cross entropy loss is calculated between the predicted output and the true output. The gradients of the loss with respect to the model parameters are then computed using backpropagation, and the optimizer updates the parameters based on the gradients. The code also prints the loss for every 10 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5d8d3326",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 1.0804\n",
      "Epoch [11/100], Loss: 0.8346\n",
      "Epoch [21/100], Loss: 0.7663\n",
      "Epoch [31/100], Loss: 0.7304\n",
      "Epoch [41/100], Loss: 0.7078\n",
      "Epoch [51/100], Loss: 0.6902\n",
      "Epoch [61/100], Loss: 0.6738\n",
      "Epoch [71/100], Loss: 0.6572\n",
      "Epoch [81/100], Loss: 0.6391\n",
      "Epoch [91/100], Loss: 0.6186\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for i in range(0, len(pt_train), batch_size):\n",
    "        batch_data = pt_train[i:i+batch_size].to(torch.float32)\n",
    "        batch_labels = qt_train[i:i+batch_size].to(torch.long)\n",
    "        inputs = rnn_utils.pad_sequence(batch_data, batch_first=True, padding_value=0).to(device) \n",
    "        lab = torch.tensor(batch_labels).to(device)\n",
    "        inputs = inputs.reshape(batch_size, -1, input_size)\n",
    "        outputs = gru(inputs)\n",
    "        loss = criterion(outputs, lab)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eada6085",
   "metadata": {},
   "source": [
    "### Here we perform validation on the test set using a trained GRU model. It loops through the test set in batches of size \"batch_size\", and for each batch, it converts the data and labels to the appropriate tensor format, pads the sequences to the max length, and feeds them into the GRU model. The outputs are then compared against the true labels, and the number of correct predictions is accumulated. Finally, the accuracy is calculated as a percentage of correctly classified samples over the total number of samples, and printed out. The \"with torch.no_grad()\" block ensures that no gradients are calculated during the validation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1c800e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the GRU model on the validation set: 0.6100\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for i in range(0, len(pt_test), batch_size):\n",
    "        batch_data = pt_test[i:i+batch_size].to(torch.float32)\n",
    "        batch_labels = qt_test[i:i+batch_size].to(torch.long)\n",
    "        inputs = nn.utils.rnn.pad_sequence(batch_data, batch_first=True, padding_value=0).to(device)\n",
    "        lab = torch.tensor(batch_labels).to(device)\n",
    "        inputs = inputs.reshape(batch_size, -1, input_size)\n",
    "        outputs = gru(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "#         print(predicted.shape)\n",
    "        total += lab.size(0)\n",
    "        correct += (predicted == lab).sum().item()\n",
    "\n",
    "accuracy = correct / total\n",
    "print('Accuracy of the GRU model on the validation set: {:.4f}'.format(accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fdc75de",
   "metadata": {},
   "source": [
    "## (5c) LSTM (Long Short-Term Memory) is a type of recurrent neural network (RNN) architecture that is designed to handle the vanishing gradient problem and capture long-term dependencies in sequential data. It achieves this through the use of a memory cell and gates that regulate the flow of information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3f8996",
   "metadata": {},
   "source": [
    "### Here we define a LSTM neural network model with a specified input size, hidden size, and output size. It also sets the learning rate, number of epochs, batch size, and device for the model. It initializes the LSTM model, criterion (loss function), and optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8b60daec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "#         self.batch_size = batch_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(1, x.size(0), self.hidden_size).to(device) \n",
    "        c0 = torch.zeros(1, x.size(0), self.hidden_size).to(device) \n",
    "        out, (hn, cn) = self.lstm(x, (h0, c0)) \n",
    "        out = self.fc(out[:, -1, :]) \n",
    "        return out\n",
    "\n",
    "input_size = 6000 \n",
    "hidden_size = 20\n",
    "output_size = 3 \n",
    "lr = 0.007\n",
    "num_epochs = 100\n",
    "batch_size = 32\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "lstm = LSTM(input_size, hidden_size, output_size).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(lstm.parameters(), lr=lr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3407adbe",
   "metadata": {},
   "source": [
    "### The model is trained for a fixed number of epochs (100) and for each epoch, the training data is divided into batches of a fixed size (32). For each batch, the model predicts the output based on the input and the current model parameters, and the cross entropy loss is calculated between the predicted output and the true output. The gradients of the loss with respect to the model parameters are then computed using backpropagation, and the optimizer updates the parameters based on the gradients. The code also prints the loss for every 10 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "418a593f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 1.0863\n",
      "Epoch [11/100], Loss: 0.9233\n",
      "Epoch [21/100], Loss: 0.8760\n",
      "Epoch [31/100], Loss: 0.8490\n",
      "Epoch [41/100], Loss: 0.8049\n",
      "Epoch [51/100], Loss: 0.7602\n",
      "Epoch [61/100], Loss: 0.7220\n",
      "Epoch [71/100], Loss: 0.6841\n",
      "Epoch [81/100], Loss: 0.6439\n",
      "Epoch [91/100], Loss: 0.6070\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for i in range(0, len(pt_train), batch_size):\n",
    "        batch_data = pt_train[i:i+batch_size].to(torch.float32)\n",
    "        batch_labels = qt_train[i:i+batch_size].to(torch.long)\n",
    "        batch_data = batch_data.reshape(batch_size, -1, input_size)\n",
    "\n",
    "        outputs = lstm(batch_data)\n",
    "\n",
    "        loss = criterion(outputs, batch_labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1f9e6f",
   "metadata": {},
   "source": [
    "### The code calculates the accuracy of the LSTM model on the test set. It loops through the test set in batches, feeds the data to the LSTM model, and calculates the number of correctly predicted labels. Finally, it calculates the accuracy by dividing the number of correctly predicted labels by the total number of labels in the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "dc2faab1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0.6176\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "lstm.eval()\n",
    "with torch.no_grad():\n",
    "    for i in range(0, len(pt_test), batch_size):\n",
    "        batch_data = pt_test[i:i+batch_size].to(torch.float32)\n",
    "        batch_labels = qt_test[i:i+batch_size].to(torch.long)\n",
    "        batch_data = batch_data.reshape(batch_size, -1, input_size)\n",
    "\n",
    "        lab = torch.tensor(batch_labels).to(device)\n",
    "\n",
    "        outputs = lstm(batch_data)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += lab.size(0)\n",
    "        correct += (predicted == lab).sum().item()\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f'Accuracy on test set: {accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5d6d6e",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "### Based on the accuracy values obtained, it appears that the LSTM outperforms both the GRU (0.6100) and simple RNN (0.6148) models with a marginally higher accuracy of 0.6176. Although the difference in accuracy between the models is relatively small, it indicates that the LSTM model may have learned the patterns and relationships in the data better than the other models. But it is worth noting that there might be another train test split where another model might outperform the LSTM. It depends on data. So, it is better if we test all the models and choose the best one out of those.\n",
    "### The reason I think LSTM performed better thatn the other models is that it can handle the vanishing gradient problem that occurs in RNNs during backpropagation. Its more complex gating mechanism allows it to selectively forget or remember information from previous time steps, making it better suited for tasks like sentiment analysis where context and temporal dependencies play a crucial role. While GRUs are faster and more efficient, the LSTM architecture was better suited for this task, resulting in higher accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
